{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/prokaj/stat-learning-2022/blob/main/week1/Lab_Ch3-Linear_Regression_in_Python.ipynb)\n",
    "    \n",
    "\n",
    "This lab on Linear Regression is a python adaptation of p. 110-120 of \"Introduction to Statistical Learning with Applications in R\" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. \n",
    "It is based on the work of R. Jordan Crouser at Smith College for SDS293: Machine Learning (Spring 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.\n",
    "\n",
    "Suppose we have a data set with five predictors, $$\\begin{aligned}\n",
    "    X_1 &= \\text{GPA},\\\\\n",
    "    X_2 &= \\text{IQ},\\\\\n",
    "    X_3 &= \\text{Level},\\quad \\text{1 for College and 0 for High\n",
    "          School},\\\\\n",
    "    X_4 &= \\text{Interaction between GPA and IQ}, \\\\\n",
    "    X_5 &= \\text{Interaction between GPA and Level}. \n",
    "  \\end{aligned}$$ The response is starting salary after graduation (in\n",
    "thousands of dollars). Suppose we use least squares to fit the model,\n",
    "and get $\\hat\\beta_0 = 50$, $\\hat \\beta_1 = 20$, $\\hat\\beta_2 = 0.07$,\n",
    "$\\hat\\beta_3 = 35$, $\\hat\\beta_4 = 0.01$, $\\hat\\beta_5 = -10$.\n",
    "\n",
    "a. Which answer is correct, and why?\n",
    "\n",
    "   -   For a fixed value of IQ and GPA, high school graduates earn\n",
    "        more, on average, than college graduates.\n",
    "\n",
    "   -   For a fixed value of IQ and GPA, college graduates earn more, on\n",
    "        average, than high school graduates.\n",
    "\n",
    "   -   For a fixed value of IQ and GPA, high school graduates earn\n",
    "        more, on average, than college graduates provided that the GPA\n",
    "        is high enough.\n",
    "\n",
    "   -   For a fixed value of IQ and GPA, college graduates earn more, on\n",
    "        average, than high school graduates provided that the GPA is\n",
    "        high enough.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.  Predict the salary of a college graduate with IQ of 110 and a GPA of\n",
    "    4.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.array([50, 20 , 0.07, 35, 0.01, -10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.  True or false: Since the coefficient for the GPA/IQ interaction term\n",
    "    is very small, there is very little evidence of an interaction\n",
    "    effect. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.\n",
    "\n",
    "This exercise relates to the College data set, which can be found in the\n",
    "file [`College.csv`](https://www.statlearning.com/s/College.csv) on the\n",
    "ISL book website. It contains a number of variables for 777 different\n",
    "universities and colleges in the US. The variables are\n",
    "\n",
    "-   `Private` : Public/private indicator\n",
    "\n",
    "-   `Apps` : Number of applications received\n",
    "\n",
    "-   `Accept` : Number of applicants accepted\n",
    "\n",
    "-   `Enroll` : Number of students enrolled\n",
    "\n",
    "-   `Top10perc` : New students from top 10 % of high school class\n",
    "\n",
    "-   `Top25perc` : New students from top 25 % of high school class\n",
    "\n",
    "-   `F.Undergrad` : Number of full-time undergraduates\n",
    "\n",
    "-   `P.Undergrad` : Number of part-time undergraduates\n",
    "\n",
    "-   `Outstate` : Out-of-state tuition\n",
    "\n",
    "-   `Room.Board` : Room and board costs\n",
    "\n",
    "-   `Books` : Estimated book costs\n",
    "\n",
    "-   `Personal` : Estimated personal spending\n",
    "\n",
    "-   `PhD` : Percent of faculty with Ph.D.'s\n",
    "\n",
    "-   `Terminal` : Percent of faculty with terminal degree\n",
    "\n",
    "-   `S.F.Ratio` : Student/faculty ratio\n",
    "\n",
    "-   `perc.alumni` : Percent of alumni who donate\n",
    "\n",
    "-   `Expend` : Instructional expenditure per student\n",
    "\n",
    "-   `Grad.Rate` : Graduation rate\n",
    "\n",
    "a.  Use the `pd.read_csv` function to load the data. Call the loaded\n",
    "    data `college`, for example\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.statlearning.com/s/College.csv\"\n",
    "\n",
    "college = pd.read_csv(url, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.datasets as smd\n",
    "\n",
    "col = smd.get_rdataset('College', 'ISLR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col.data, col.package, col.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.  Look at the data using the `head` method. You should notice that the\n",
    "    row index is just the name of each university. This is the effect of\n",
    "    `index_col=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " college.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4effddbc",
   "metadata": {
    "id": "4effddbc"
   },
   "source": [
    "c.  Use the `describe` function to produce a numerical summary of the\n",
    "    variables in the data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccadf51a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1636097563679,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "ccadf51a",
    "outputId": "6656fe0f-cf67-448d-8e8d-a38b7a3bc337"
   },
   "outputs": [],
   "source": [
    "    college.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb3eef",
   "metadata": {
    "id": "9abb3eef",
    "lines_to_next_cell": 0
   },
   "source": [
    "d. Use the `pair_plot` function from the `seaborn` package to produce a\n",
    "   scatterplot matrix of the first ten columns or variables of the\n",
    "   data. Recall that you can reference the first ten columns of a\n",
    "   `pandas.DataFrame` by applying the `.iloc(1)[:10]` method on it.\n",
    "\n",
    "   Note that `Private` is missing from the plot. Why?\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qLqTMgdRDA28",
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1636098158232,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "qLqTMgdRDA28"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3746f33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 794
    },
    "executionInfo": {
     "elapsed": 53820,
     "status": "ok",
     "timestamp": 1636097876029,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "a3746f33",
    "outputId": "184e9faa-bbb3-4e07-d65a-44290f21cc4d"
   },
   "outputs": [],
   "source": [
    "\n",
    "smalldf = college.iloc(1)[:10]\n",
    "smalldf['Private'] = 1*(smalldf.Private=='Yes')\n",
    "sns.pairplot(smalldf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093b386f",
   "metadata": {
    "id": "093b386f"
   },
   "source": [
    "e.  Use the `seaborn.boxplot` function to produce side-by-side boxplots\n",
    "    of `Outstate` versus `Private`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c546ffbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1636097939187,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "c546ffbe",
    "outputId": "ad9825bc-2abb-4131-e65c-fbe7e5f33a26"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='Private', y='Outstate', data=college) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d3b25",
   "metadata": {
    "id": "218d3b25",
    "lines_to_next_cell": 0
   },
   "source": [
    "f. Create a new qualitative variable, called `Elite`, by binning the\n",
    "   `Top10perc` variable. We are going to divide universities into two\n",
    "   groups based on whether or not the proportion of students coming\n",
    "   from the top 10 % of their high school classes exceeds 50 %.\n",
    "\n",
    "        college[\"Elite\"] = college.Top10perc>50\n",
    "\n",
    "   Use the `.value_counts` method to see how many elite universities\n",
    "   there are. Now use the `boxplot()` function from the `seaborn`\n",
    "   package to produce side-by-side boxplots of `Outstate` versus\n",
    "   `Elite`.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1de255",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1636098163734,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "ef1de255",
    "outputId": "372ab463-56c3-4da3-da45-104d34abe0ac"
   },
   "outputs": [],
   "source": [
    " college[\"Elite\"] = college.Top10perc>50\n",
    " print(college.Elite.value_counts())\n",
    " sns.boxplot(x='Elite', y='Outstate', data=college)\n",
    " plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3833c3",
   "metadata": {
    "id": "3c3833c3"
   },
   "source": [
    "g.  Use the `.hist` method to produce some histograms with differing\n",
    "    numbers of bins for a few of the quantitative variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5b9c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 844
    },
    "executionInfo": {
     "elapsed": 1072,
     "status": "ok",
     "timestamp": 1636098555897,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "43a5b9c6",
    "outputId": "5bce5c3a-862f-4a34-df51-ef4f8a434151"
   },
   "outputs": [],
   "source": [
    "college.hist('Accept', bins=51)\n",
    "college.hist('Room.Board', bins=21)\n",
    "college[college.Private!='Yes'].hist('Outstate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2feeff",
   "metadata": {
    "id": "2a2feeff"
   },
   "source": [
    "h.  Continue exploring the data, and provide a brief summary of what you\n",
    "    discover.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90f1a52",
   "metadata": {
    "id": "d90f1a52"
   },
   "source": [
    "## Problem 3.\n",
    "\n",
    "This exercise involves the\n",
    "[`Auto`](https://www.statlearning.com/s/Auto.csv) data set. Make sure\n",
    "that the missing values have been removed from the data, e.g.\n",
    "\n",
    "    #----------------\n",
    "    import statsmodels.datasets as sm_datasets\n",
    "    auto = sm_datasets.get_rdataset('Auto', 'ISLR')\n",
    "    auto.data\n",
    "    #----------------\n",
    "\n",
    "or from the web page\n",
    "\n",
    "    #----------------\n",
    "    from urllib.request import urlopen\n",
    "    import pandas as pd\n",
    "    url = 'https://www.statlearning.com/s/Auto.csv'\n",
    "    auto = pd.read_csv(url, na_values='?').dropna()\n",
    "    auto\n",
    "\n",
    "a.  Which of the predictors are quantitative, and which are qualitative?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85abb76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1636104266610,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "e85abb76",
    "outputId": "5ffb521c-c394-4426-a16d-aa3dfe9e1b3c"
   },
   "outputs": [],
   "source": [
    "import statsmodels.datasets as sm_datasets\n",
    "auto = sm_datasets.get_rdataset('Auto', 'ISLR')\n",
    "# auto.data = auto.data.set_index('name').copy()\n",
    "auto.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U0SHK-REGJU5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1636104175843,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "U0SHK-REGJU5",
    "outputId": "293e54a2-52eb-4af8-dc7c-b6415fd52b09"
   },
   "outputs": [],
   "source": [
    "auto?\n",
    "auto.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DB9byzMlHBKj",
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1636099173053,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "DB9byzMlHBKj"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eDpLHVuOGgaH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1636104179913,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "eDpLHVuOGgaH",
    "outputId": "4301d2b7-5b7a-484b-de55-ec135b513804"
   },
   "outputs": [],
   "source": [
    "disp = auto.data.displacement*(2.5**3)\n",
    "disp.min(), disp.max(), disp.mean(), disp.std(), np.percentile(disp, [25, 50, 75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825064b",
   "metadata": {
    "id": "0825064b"
   },
   "source": [
    "b.  What is the range (minimum, maximum) of each quantitative predictor?\n",
    "    You can answer this using the `min`, `max` functions or write a\n",
    "    little function, called `range`, which returns this tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8120bdcf",
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1636104187751,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "8120bdcf"
   },
   "outputs": [],
   "source": [
    "def minmax(x, axis=None):\n",
    "    return np.min(x, axis=axis), np.max(x, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sU-qNqZsH-oY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1636104190235,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "sU-qNqZsH-oY",
    "outputId": "bd17aa6d-10fa-4cac-c121-5bc4a465ec9f"
   },
   "outputs": [],
   "source": [
    "minmax(auto.data, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fcce12",
   "metadata": {
    "id": "57fcce12"
   },
   "source": [
    "c.  What is the mean and standard deviation of each quantitative\n",
    "    predictor?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e0e39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1636104492466,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "db1e0e39",
    "outputId": "a45d2166-2955-4317-a7c8-bc271a5df55b"
   },
   "outputs": [],
   "source": [
    "print(auto.data.columns)\n",
    "quant = ['mpg', 'displacement', 'horsepower', 'weight', 'acceleration', 'year']\n",
    "{k: (auto.data[k].mean(), auto.data[k].std()) for k in quant}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e018e",
   "metadata": {
    "id": "574e018e"
   },
   "source": [
    "d.  Now remove the 10th through 85th observations. What is the range,\n",
    "    mean, and standard deviation of each predictor in the subset of the\n",
    "    data that remains?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YWanKU7sLslc",
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1636104197420,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "YWanKU7sLslc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fe2dba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 779,
     "status": "ok",
     "timestamp": 1636104278994,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "21fe2dba",
    "outputId": "a2cece21-920b-4a23-b055-2d9aed48a35f"
   },
   "outputs": [],
   "source": [
    "idx = np.arange(len(auto.data))\n",
    "idx = (idx<10)+(idx>85)\n",
    "auto1 = auto.data[idx]\n",
    "plt.plot(auto1[quant].mean(axis=0),'o')\n",
    "plt.plot(auto.data[quant].mean(axis=0),'o',alpha=0.5)\n",
    "plt.title('mean')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(auto1[quant].min(axis=0),'o')\n",
    "plt.plot(auto.data[quant].min(axis=0),'o',alpha=0.5)\n",
    "plt.title('min')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5286287a",
   "metadata": {
    "id": "5286287a"
   },
   "source": [
    "e.  Using the full data set, investigate the predictors graphically,\n",
    "    using scatterplots or other tools of your choice. Create some plots\n",
    "    highlighting the relationships among the predictors. Comment on your\n",
    "    findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fJTvzn8VMwDq",
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1636104282633,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "fJTvzn8VMwDq"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97521e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 759
    },
    "executionInfo": {
     "elapsed": 18380,
     "status": "ok",
     "timestamp": 1636104303073,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "2c97521e",
    "outputId": "f89364a6-169f-435a-9cba-fb8521bd7563"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(auto.data[quant])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zHIUXPJ_QQaD",
   "metadata": {
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1636104309781,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "zHIUXPJ_QQaD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jVwPU83-OuuK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 679,
     "status": "ok",
     "timestamp": 1636104312805,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "jVwPU83-OuuK",
    "outputId": "40b837e8-acab-4fb0-f145-2d629db4e42a"
   },
   "outputs": [],
   "source": [
    "cyl_yr = auto.data.groupby(['cylinders', 'year']).size()\n",
    "col_yr_cnt = cyl_yr.values\n",
    "col_yr = np.array(list(cyl_yr.index))\n",
    "col_yr = pd.DataFrame(col_yr, columns=['cylinders', 'year'])\n",
    "col_yr['count'] = col_yr_cnt\n",
    "sns.scatterplot(x='year', y='cylinders', size='count', \n",
    "                hue='count', sizes=(10, 250), legend=False, data=col_yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660bf78",
   "metadata": {
    "id": "e660bf78"
   },
   "source": [
    "f.  Suppose that we wish to predict gas mileage (`mpg`) on the basis of\n",
    "    the other variables. Do your plots suggest that any of the other\n",
    "    variables might be useful in predicting `mpg`? Justify your answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36080148",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 701,
     "status": "ok",
     "timestamp": 1636104319120,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "36080148",
    "outputId": "ef81aa4d-7990-4d5f-a7c1-c7fd264bc12b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(auto.data.drop(columns=['cylinders', 'origin']).corr(), annot=True, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Gydj733hVPR6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1636104323702,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "Gydj733hVPR6",
    "outputId": "e5318d11-7779-4dc5-e7b7-4f371bfc8e37"
   },
   "outputs": [],
   "source": [
    "print(auto.data.cylinders.value_counts())\n",
    "print(auto.data.origin.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wnP2fGt3U1vb",
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1636104327287,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "wnP2fGt3U1vb"
   },
   "outputs": [],
   "source": [
    "for level in auto.data.cylinders.unique():\n",
    "    if level!=4:\n",
    "        auto.data[f'cyl_{level}'] = 1*(auto.data.cylinders==level)\n",
    "\n",
    "for level in auto.data.origin.unique():\n",
    "    if level!=1:\n",
    "        auto.data[f'orig_{level}'] = 1*(auto.data.origin==level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-piRPvnkWuGi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 670
    },
    "executionInfo": {
     "elapsed": 1468,
     "status": "ok",
     "timestamp": 1636104408946,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "-piRPvnkWuGi",
    "outputId": "bac04233-fbf1-4d8a-b304-670beb2269ec"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(auto.data.drop(columns=['cylinders', 'origin']).corr(), \n",
    "            annot=True, vmin=-1, vmax=1,\n",
    "            cmap = sns.diverging_palette(220, 20, n=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Section 3.6 from ISL\n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Tells matplotlib to display images inline instead of a new window\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing the data from [Boston.csv](http://www.science.smith.edu/~jcrouser/SDS293/data/Boston.csv) into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from_smith_edu = False # or True \n",
    "\n",
    "if not from_smith_edu:\n",
    "    from sklearn.datasets import load_boston\n",
    "    ds = load_boston()\n",
    "    colnames = [line.split()[1].lower() for line in ds['DESCR'].split('\\n') if \"    -\" in line]\n",
    "    vars = np.concatenate([ds.data, ds.target[:,None]], axis=-1)\n",
    "    df = pd.DataFrame(vars, columns=colnames)\n",
    "    df.__doc__=ds['DESCR']\n",
    "else:\n",
    "    ## alternatively we could use this:\n",
    "    url = 'https://www.science.smith.edu/~jcrouser/SDS293/data/Boston.csv'\n",
    "    df = pd.read_csv(url, index_col=0)\n",
    "\n",
    "print(df.head())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6.2 Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit a simple linear model (`OLS` - for \"ordinary least squares\" method) with `medv` as the response and `lstat` as the predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lm = sm.OLS.from_formula('medv ~ lstat', df)\n",
    "result = lm.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get detailed information about the model, we can print the results of a call to the `.summary()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(result.summary().as_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want individual attributes? You can access them independently like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result.rsquared, result.fvalue, result.params.Intercept, result.params.lstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a complete list of attributes and methods of a `RegressionResults` object, see: http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.RegressionResults.html?highlight=regressionresults\n",
    "\n",
    "For quick help use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try making some predictions using this model. First, we'll set up a dataframe containing the `lstat` values for which we want to predict a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "new = pd.DataFrame([[1, 5], [1, 10], [1, 15]], columns=['Intercept', 'lstat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just call the `.predict()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.predict(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically those are the right prediction values, but maybe it would be good to have the confidence intervals along with them. \n",
    "\n",
    "Recall that, when the linear model describes the response correctly\n",
    "$$\n",
    "    \\hat\\beta = (X^TX)^{-1}X^Ty = (X^TX)^{-1}X^T (X \\beta+\\varepsilon)=\\beta + (X^TX)^{-1}X^T \\varepsilon\\sim N(\\beta, \\sigma^2(X^TX)^{-1}) \n",
    "$$\n",
    "\n",
    "When we apply the model at a new observation $x$, then\n",
    "$$\n",
    "    \\hat{y} = x\\hat\\beta = x\\beta + x(\\hat\\beta-\\beta)\n",
    "$$\n",
    "Here $x\\beta$ is true mean of the unobserved response $y$ that belongs to $x$.\n",
    "Therefore\n",
    "$$\n",
    "    y = x\\beta+\\varepsilon' = \\hat y + x(\\beta-\\hat\\beta) + \\varepsilon' \n",
    "$$\n",
    "Here $\\beta-\\hat\\beta$ and $\\varepsilon$ are independent noise so overall the variance of $y-\\hat y$ is \n",
    "$x\\Sigma(\\hat\\beta)x^T+\\sigma^2 = \\sigma^2(x(X^TX)^{-1}x^T+1)$ \n",
    "$\\sigma^2$ is unknown it is estimated from the data.\n",
    "\n",
    "$$\n",
    "\\frac{y-\\hat y}{\\hat\\sigma \\sqrt{1+x(X^TX)^{-1}x^T+1}}=\\frac{y-\\hat y}{\\hat\\sigma_0}\\sim t_{n-(p+1)}\n",
    "$$\n",
    "\n",
    "From this\n",
    "$$\n",
    "    \\mathbb{P}(y \\in (\\hat y-(t_\\alpha/2)\\hat\\sigma_0, \\hat y+(t_\\alpha/2)\\hat\\sigma_0))=\\alpha,\\quad \\text{where $t_{\\alpha/2}$ is the critical value.} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Sigma=result.cov_params()\n",
    "(new.dot(Sigma)*new).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.cov_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance of the `intercept` term and the `lstat` coefficient is negative. Student-t distribution is in the `scipy.stats` module.\n",
    "Let's write a little helper function to get that and package it all up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.stats import t as student_t\n",
    "def predict(res, new, alpha=0.05):\n",
    "    fit = pd.DataFrame(res.predict(new), columns=['fit'])\n",
    "    # the variance of the predicted value is <new Sigma, new> (new is a row vector).\n",
    "    sigma = (new.dot(res.cov_params())*new).sum(axis=1)**0.5\n",
    "    # isf is the inverse survival function, \n",
    "    # that is the inverse of 1-F, where F is the distribution function\n",
    "    t = student_t(df=res.df_resid, scale=sigma).isf(alpha/2)\n",
    "    fit['lower'] = fit.fit-t\n",
    "    fit['upper'] = fit.fit+t\n",
    "    return fit\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a confidence in the expected value of the response (the value of the regression function). Since new observations at these levels of `lstat` also contains the noise term, the confidence intervals for the new observed response values are much higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(res, new, interval='confidence', alpha=0.05):\n",
    "    fit = pd.DataFrame(res.predict(new), columns=['fit'])\n",
    "    sigma2 = (new.dot(res.cov_params())*new).sum(axis=1)\n",
    "    if interval=='prediction':\n",
    "        sigma2 += res.scale\n",
    "    sigma = sigma2**.5\n",
    "    t = student_t(df=res.df_resid, scale=sigma).isf(alpha/2)\n",
    "    fit['lower'] = fit.fit-t\n",
    "    fit['upper'] = fit.fit+t\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(result, new)) \n",
    "print(predict(result, new, interval='prediction')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn is a Python visualization library based on matplotlib that provides a high-level interface for drawing attractive statistical graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot `medv` and `lstat` along with the least squares regression line using the `regplot()` function. We can define the color of the fit line using `line_kws` (\"line keywords\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.regplot(x='lstat', y='medv', data=df, line_kws = {\"color\": \"r\"}, ci=None)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the residuals against the fitted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({'Fitted values': result.fittedvalues, 'Residuals': result.resid})\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.regplot(x='Fitted values', y='Residuals', data=result_df, fit_reg=False)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we want studentized residuals instead? (This is almost the same as the residuals normalized to have unit norm, the latter is available as `.resid_pearson`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=np.arange(result.nobs), \n",
    "                y=result.get_influence().resid_studentized-result.resid_pearson)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_df['S. Residuals'] = result.get_influence().resid_studentized\n",
    "sns.regplot(x='Fitted values', y='S. Residuals', data=result_df,  fit_reg=False)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look for points with high leverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "result_df['Leverage'] = OLSInfluence(result).hat_matrix_diag\n",
    "sns.regplot(x='Leverage', y='S. Residuals', data=result_df,  fit_reg=False)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.6.3 Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit a multiple linear regression model using least squares, we again use the `from_formula()` function. The syntax `from_formula(y∼x1+x2+x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`. The `summary()` function now outputs the regression coefficients for all the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = sm.OLS.from_formula('medv ~ lstat + age', df)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston data set contains 13 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# All columns (except medv, which is our response)\n",
    "model = sm.OLS.from_formula('medv ~ ' + '+'.join(df.columns.difference(['medv'])), df)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used the syntax `.join(df.columns.difference(['medv']))` to exclude the response variable above. We can use the same napproach to perform a regression using just a subset of the predictors? For example, in the above regression output, `age` and `indus` have a high p-value. So we may wish to run a regression excluding these predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# All columns (except medv age and indus)\n",
    "model = sm.OLS.from_formula('medv ~ ' + '+'.join(df.columns.difference(['medv', 'age', 'indus'])), df)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.6.4 Interaction Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to include interaction terms in a linear model using the `.from_formula()` function. The syntax `lstat:black` tells Python to include an interaction term between `lstat` and `black`. The syntax `lstat*age` simultaneously includes `lstat`, `age`, and the interaction term `lstat*age` as predictors; it is a shorthand for `lstat+age+lstat:age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(sm.OLS.from_formula('medv ~ lstat*age', df).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.6.5 Non-linear Transformations of the Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.from_formula()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor `X`, we can create a predictor `X^2` using `np.square(X)`. We now perform a regression of `medv` onto `lstat` and `lstat^2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lm.fit2 = sm.OLS.from_formula('medv ~ lstat + np.square(lstat)', df).fit()\n",
    "print(lm.fit2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The near-zero p-value associated with the quadratic term suggests that it leads to an improved model. We use the `anova_lm()` function to further quantify the extent to which the quadratic fit is superior to the linear fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lm.fit = sm.OLS.from_formula('medv ~ lstat', df).fit()\n",
    "print(sm.stats.anova_lm(lm.fit, lm.fit2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Model 0 represents the linear submodel containing only one predictor, `lstat`, while Model 1 corresponds to the larger quadraticmodel that has two predictors, `lstat` and `lstat2`. The `anova_lm()` function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. \n",
    "\n",
    "The F-statistic is 135 and the associated p-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat2` is far superior to the model that only contains the predictor `lstat`. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. \n",
    "\n",
    "If we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fitted_values = pd.Series(lm.fit2.fittedvalues, name=\"Fitted Values\")\n",
    "residuals = pd.Series(lm.fit2.get_influence().resid_studentized, name=\"S. Residuals\")\n",
    "sns.regplot(x=fitted_values, y=s_residuals,  fit_reg=False)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we see that when the `lstat2` term is included in the model, there is little discernible pattern in the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a cubic fit, we can include a predictor of the form `np.power(x, 3))`. However, this approach can start to get cumbersome for higher order polynomials. A better approach involves using list comprehension inside a `.join()`. For example, the following command produces a fifth-order polynomial fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "formula_5 = 'medv ~ ' + '+'.join(['np.power(lstat,' + str(i) + ')' for i in range(1,6)])\n",
    "print(formula_5)\n",
    "print(sm.OLS.from_formula(formula_5, df).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(sm.OLS.from_formula('medv ~ np.log(rm)', df).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.6.6 Qualitative Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now examine the [`Carseats`](http://www.science.smith.edu/~jcrouser/SDS293/data/Carseats.csv) data that we talked about earlier in class. We will attempt to predict `Sales` (child car seat sales) in 400 locations based on a number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if from_smith_edu:\n",
    "    df2 = pd.read_csv('Carseats.csv')\n",
    "else:\n",
    "    import statsmodels.datasets as smd\n",
    "    df2 = smd.get_rdataset('Carseats', 'ISLR')\n",
    "    df2.data.__doc__= df2.__doc__\n",
    "    df2 = df2.data\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Carseats` data includes qualitative predictors such as `Shelveloc`, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor `Shelveloc` takes on three possible values, `Bad`, `Medium`, and `Good`.\n",
    "\n",
    "Given a qualitative variable such as `Shelveloc`, Python generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "formula_carseat = 'Sales ~ Income:Advertising+Price:Age + ' + \" + \".join(df2.columns.difference(['Sales']))\n",
    "print(formula_carseat)\n",
    "print(sm.OLS.from_formula(formula_carseat, df2).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn how to set other coding schemes (or _contrasts_), see: http://statsmodels.sourceforge.net/devel/examples/notebooks/generated/contrasts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormaltest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'propagate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mnormaltest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'propagate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Test whether a sample differs from a normal distribution.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This function tests the null hypothesis that a sample comes\u001b[0m\n",
      "\u001b[0;34m    from a normal distribution.  It is based on D'Agostino and\u001b[0m\n",
      "\u001b[0;34m    Pearson's [1]_, [2]_ test that combines skew and kurtosis to\u001b[0m\n",
      "\u001b[0;34m    produce an omnibus test of normality.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Parameters\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    a : array_like\u001b[0m\n",
      "\u001b[0;34m        The array containing the sample to be tested.\u001b[0m\n",
      "\u001b[0;34m    axis : int or None, optional\u001b[0m\n",
      "\u001b[0;34m        Axis along which to compute test. Default is 0. If None,\u001b[0m\n",
      "\u001b[0;34m        compute over the whole array `a`.\u001b[0m\n",
      "\u001b[0;34m    nan_policy : {'propagate', 'raise', 'omit'}, optional\u001b[0m\n",
      "\u001b[0;34m        Defines how to handle when input contains nan.\u001b[0m\n",
      "\u001b[0;34m        The following options are available (default is 'propagate'):\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m          * 'propagate': returns nan\u001b[0m\n",
      "\u001b[0;34m          * 'raise': throws an error\u001b[0m\n",
      "\u001b[0;34m          * 'omit': performs the calculations ignoring nan values\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns\u001b[0m\n",
      "\u001b[0;34m    -------\u001b[0m\n",
      "\u001b[0;34m    statistic : float or array\u001b[0m\n",
      "\u001b[0;34m        ``s^2 + k^2``, where ``s`` is the z-score returned by `skewtest` and\u001b[0m\n",
      "\u001b[0;34m        ``k`` is the z-score returned by `kurtosistest`.\u001b[0m\n",
      "\u001b[0;34m    pvalue : float or array\u001b[0m\n",
      "\u001b[0;34m       A 2-sided chi squared probability for the hypothesis test.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    References\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    .. [1] D'Agostino, R. B. (1971), \"An omnibus test of normality for\u001b[0m\n",
      "\u001b[0;34m           moderate and large sample size\", Biometrika, 58, 341-348\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    .. [2] D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from\u001b[0m\n",
      "\u001b[0;34m           normality\", Biometrika, 60, 613-622\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Examples\u001b[0m\n",
      "\u001b[0;34m    --------\u001b[0m\n",
      "\u001b[0;34m    >>> from scipy import stats\u001b[0m\n",
      "\u001b[0;34m    >>> rng = np.random.default_rng()\u001b[0m\n",
      "\u001b[0;34m    >>> pts = 1000\u001b[0m\n",
      "\u001b[0;34m    >>> a = rng.normal(0, 1, size=pts)\u001b[0m\n",
      "\u001b[0;34m    >>> b = rng.normal(2, 1, size=pts)\u001b[0m\n",
      "\u001b[0;34m    >>> x = np.concatenate((a, b))\u001b[0m\n",
      "\u001b[0;34m    >>> k2, p = stats.normaltest(x)\u001b[0m\n",
      "\u001b[0;34m    >>> alpha = 1e-3\u001b[0m\n",
      "\u001b[0;34m    >>> print(\"p = {:g}\".format(p))\u001b[0m\n",
      "\u001b[0;34m    p = 8.4713e-19\u001b[0m\n",
      "\u001b[0;34m    >>> if p < alpha:  # null hypothesis: x comes from a normal distribution\u001b[0m\n",
      "\u001b[0;34m    ...     print(\"The null hypothesis can be rejected\")\u001b[0m\n",
      "\u001b[0;34m    ... else:\u001b[0m\n",
      "\u001b[0;34m    ...     print(\"The null hypothesis cannot be rejected\")\u001b[0m\n",
      "\u001b[0;34m    The null hypothesis can be rejected\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_chk_asarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcontains_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_contains_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mcontains_nan\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnan_policy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'omit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormaltest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskewtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkurtosistest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mk2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mNormaltestResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/scipy/stats/_stats_py.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "scipy.stats.normaltest??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d8b53ac59aeb7dd99955ed1c7af180cec0b2adbd03b0a8162d4077beb5f10154"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
